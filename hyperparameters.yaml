# Optuna Hyperparameter Search Configuration

study:
  name: "VCR-Mobile-Optimization" # New study for mobile backbones
  direction: "maximize" # maximize val_acc
  n_trials: 600
  timeout: null # seconds, null = unlimited

  # Optuna sampler configuration
  sampler:
    type: "TPE" # Tree-structured Parzen Estimator
    # Alternative: "Random", "Grid", "CmaEs"

  # Optuna pruner (early stopping for bad trials)
  pruner:
    type: "MedianPruner"
    n_startup_trials: 5
    n_warmup_steps: 10
    # Alternative: "HyperbandPruner", "PercentilePruner", null (no pruning)

# Hyperparameters to optimize
# Format: name: {type, ...type-specific params}
hyperparameters:
  # Model architecture
  backbone:
    type: "categorical"
    choices: [
        "colornet_v1", # 1.5M params (Our Star)
        "mobilenetv4_small", # 4.5M params (Strong baseline)
        "fastvit_t8", # 6.3M params (Fastest inference)
        "resnet18", # 11M params (Robust standard)
        "efficientnet_b0", # 6.6M params (High accuracy)
      ]

  fusion:
    type: "categorical"
    choices: ["msff", "global_concat", "simple_concat"]

  loss_fn:
    type: "categorical"
    choices: ["smooth_modulation", "focal"]

  # Optimizer
  lr:
    type: "float"
    low: 1.0e-5
    high: 1.0e-3
    log: true # search in log scale

  weight_decay:
    type: "float"
    low: 1.0e-6
    high: 1.0e-3
    log: true

  batch_size:
    type: "categorical"
    choices: [32, 64] # 16 is usually too small for batch norm stability

  # Data augmentation
  image_size:
    type: "categorical"
    choices: [224, 256] # 288 slows down training significantly

# Fixed parameters (not optimized)
fixed:
  epochs: 200 # User requested keeping this high given early stopping
  use_weighted_sampler: true
  device: "auto"
  experiment_name: "VCR-Optimization"
