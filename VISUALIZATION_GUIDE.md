# ğŸ“Š Optimization Results Visualization Guide

This guide explains how to visualize and analyze the results generated by the hyperparameter optimization step (`05_optimize.py`).

## ğŸ“‚ Generated Artifacts

After running optimization, the following artifacts are created:

```
runs/
â”œâ”€â”€ experiments.db                    # SQLite database with all trial data
â””â”€â”€ {experiment_name}/
    â”œâ”€â”€ optimize/
    â”‚   â””â”€â”€ study.pkl                 # Pickled Optuna study object
    â”œâ”€â”€ best_configs/                 # Best hyperparameters per backbone
    â”‚   â”œâ”€â”€ resnet50_best.yaml
    â”‚   â”œâ”€â”€ efficientnet_b0_best.yaml
    â”‚   â”œâ”€â”€ mobilenetv4_small_best.yaml
    â”‚   â””â”€â”€ ...
    â””â”€â”€ trial_{N}/                    # Individual trial outputs
        â”œâ”€â”€ train/
        â”‚   â”œâ”€â”€ checkpoints/
        â”‚   â”œâ”€â”€ metrics.json          # Training metrics for each trial
        â”‚   â””â”€â”€ config.yaml           # Configuration used in trial
        â””â”€â”€ ...

mlruns/
â””â”€â”€ {experiment_id}/
    â”œâ”€â”€ {run_id}/                     # Parent optimization run
    â”‚   â”œâ”€â”€ metrics/                  # Overall optimization metrics
    â”‚   â”œâ”€â”€ params/                   # Study configuration
    â”‚   â””â”€â”€ tags/
    â””â”€â”€ {nested_run_id}/              # Individual trial runs (nested)
        â”œâ”€â”€ metrics/                  # Trial metrics (val_acc, trial_number)
        â”œâ”€â”€ params/                   # Trial hyperparameters
        â””â”€â”€ tags/                     # Trial state (completed, pruned, failed)
```

## ğŸ” Visualization Methods

### 1. MLflow UI (Recommended)

MLflow provides a comprehensive web interface for exploring optimization results.

#### Start MLflow UI

```bash
# From project root
mlflow ui

# Or specify custom port
mlflow ui --port 5001

# Or specify backend store
mlflow ui --backend-store-uri file:///absolute/path/to/mlruns
```

Access the UI at: `http://localhost:5000`

#### Navigate the Results

1. **Overview Page**: See all experiments
2. **Experiment View**: 
   - Click on your optimization experiment (e.g., "VCR-Optimization")
   - View all trials (nested runs)
   - Sort by metrics (val_acc, trial_number)
3. **Compare Trials**:
   - Select multiple trials using checkboxes
   - Click "Compare" button
   - View parallel coordinates plot, scatter plots, and parameter comparison
4. **Filter and Search**:
   - Use filters: `params.backbone = "resnet50"` 
   - Sort by: `metrics.val_acc DESC`
   - Search specific trial numbers

#### Visualization Features in MLflow

- **Parallel Coordinates Plot**: Compare hyperparameters across trials
- **Scatter Plot Matrix**: Correlations between parameters and metrics
- **Metric History**: Training curves for each trial
- **Parameter Importance**: Visual ranking (if logged)

### 2. Optuna Dashboard

Use Optuna's built-in visualization dashboard for interactive exploration.

#### Launch Dashboard

```bash
# Install optuna-dashboard if not already installed
pip install optuna-dashboard

# Launch dashboard
optuna-dashboard sqlite:///runs/experiments.db
```

Access at: `http://localhost:8080`

#### Features

- **Optimization History**: Timeline of trial values
- **Parameter Importance**: Statistical analysis of which hyperparameters matter most
- **Parallel Coordinate Plot**: Multi-dimensional visualization
- **Slice Plot**: How each parameter affects the objective
- **Contour Plot**: 2D relationships between parameters
- **Intermediate Values**: Pruning visualization

### 3. Python Scripts (Programmatic Access)

#### Analyze Study with CLI Tool

The project includes `analyze_study.py` for quick analysis:

```bash
# Auto-detect and analyze
python analyze_study.py

# Specify database
python analyze_study.py --db runs/experiments.db

# Save best configs per backbone
python analyze_study.py --save-configs
```

**Output includes:**
- Study overview (number of trials, states)
- Overall best trial
- Best hyperparameters per backbone
- Summary table of all trials
- Saved YAML configs for best models

#### Load and Query Study Object (PKL)

```python
import joblib
from pathlib import Path

# Load study from pickle file
study_path = Path("runs/{experiment_name}/optimize/study.pkl")
study = joblib.load(study_path)

# Get best trial
best_trial = study.best_trial
print(f"Best value: {best_trial.value:.4f}")
print(f"Best params: {best_trial.params}")

# Get all completed trials
completed_trials = [t for t in study.trials if t.state.name == 'COMPLETE']
print(f"Completed trials: {len(completed_trials)}")

# Filter by backbone
resnet_trials = [t for t in completed_trials if t.params.get('backbone') == 'resnet50']
best_resnet = max(resnet_trials, key=lambda t: t.value)
print(f"Best ResNet50 trial: {best_resnet.number}, score: {best_resnet.value:.4f}")

# Access trial details
for trial in study.trials[:5]:  # First 5 trials
    print(f"Trial {trial.number}: {trial.state.name}")
    print(f"  Params: {trial.params}")
    print(f"  Value: {trial.value}")
    print(f"  Duration: {trial.duration}")
```

#### Load from SQLite Database

```python
import optuna

# Connect to database
storage_url = "sqlite:///runs/experiments.db"
study_name = "VCR-Optimization"  # Your study name

# Load study
study = optuna.load_study(study_name=study_name, storage=storage_url)

# List all studies in database
summaries = optuna.study.get_all_study_summaries(storage_url)
for summary in summaries:
    print(f"Study: {summary.study_name}, Trials: {summary.n_trials}")
```

#### Generate Custom Visualizations

```python
import optuna
from optuna.visualization import (
    plot_optimization_history,
    plot_param_importances,
    plot_parallel_coordinate,
    plot_slice,
    plot_contour,
    plot_intermediate_values,
)

# Load study
study = optuna.load_study(study_name="VCR-Optimization", 
                          storage="sqlite:///runs/experiments.db")

# Create visualizations (returns Plotly figures)
fig1 = plot_optimization_history(study)
fig1.write_html("optimization_history.html")

fig2 = plot_param_importances(study)
fig2.write_html("param_importances.html")

fig3 = plot_parallel_coordinate(study)
fig3.write_html("parallel_coordinate.html")

fig4 = plot_slice(study)
fig4.write_html("slice_plot.html")

# Contour plot for specific parameters
fig5 = plot_contour(study, params=['lr', 'batch_size'])
fig5.write_html("contour_lr_batch.html")

# Show in browser
fig1.show()
```

### 4. Read Trial Metrics (JSON)

Each trial saves detailed metrics in JSON format:

```python
import json
from pathlib import Path

# Load metrics for a specific trial
trial_dir = Path("runs/{experiment_name}/trial_5/train")
metrics_path = trial_dir / "metrics.json"

with open(metrics_path, 'r') as f:
    metrics = json.load(f)

# Access metrics
print(f"Best validation accuracy: {metrics['best_val_acc']:.4f}")
print(f"Best epoch: {metrics['best_epoch']}")
print(f"Final train loss: {metrics['train_loss'][-1]:.4f}")
print(f"Training time: {metrics.get('training_time_seconds', 'N/A')}")

# Plot learning curves
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(metrics['train_loss'], label='Train')
plt.plot(metrics['val_loss'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss Curves')

plt.subplot(1, 2, 2)
plt.plot(metrics['train_acc'], label='Train')
plt.plot(metrics['val_acc'], label='Val')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy Curves')

plt.tight_layout()
plt.savefig('trial_5_curves.png')
plt.show()
```

### 5. Use Best Configurations

The optimization automatically saves the best configuration for each backbone:

```bash
# Train final model with best hyperparameters
python 06_train.py \
    --config runs/{experiment_name}/best_configs/resnet50_best.yaml \
    --experiment final_resnet50
```

Or load programmatically:

```python
import yaml
from pathlib import Path

# Load best config
config_path = Path("runs/{experiment_name}/best_configs/resnet50_best.yaml")
with open(config_path, 'r') as f:
    best_config = yaml.safe_load(f)

print(f"Best backbone: {best_config['training']['backbone']}")
print(f"Best learning rate: {best_config['training']['lr']}")
print(f"Trial number: {best_config['metadata']['trial_number']}")
print(f"Score achieved: {best_config['metadata']['best_score']:.4f}")
```

## ğŸ“ˆ Recommended Analysis Workflow

1. **Quick Overview**:
   ```bash
   python analyze_study.py --save-configs
   ```

2. **Interactive Exploration**:
   ```bash
   # Terminal 1: MLflow
   mlflow ui
   
   # Terminal 2: Optuna Dashboard (optional)
   optuna-dashboard sqlite:///runs/experiments.db
   ```

3. **Deep Dive Analysis**:
   - Use Optuna visualizations to understand parameter importance
   - Compare top 5-10 trials in MLflow
   - Check learning curves in trial metrics
   - Analyze per-backbone performance

4. **Extract Best Models**:
   - Use saved best configs from `best_configs/`
   - Re-train with best hyperparameters for final evaluation

## ğŸ”§ Common Queries

### Find all trials with specific backbone
```python
study = joblib.load("runs/{experiment}/optimize/study.pkl")
resnet_trials = [t for t in study.trials 
                 if t.params.get('backbone') == 'resnet50' 
                 and t.state.name == 'COMPLETE']
```

### Get top N trials
```python
completed = [t for t in study.trials if t.state.name == 'COMPLETE']
top_5 = sorted(completed, key=lambda t: t.value, reverse=True)[:5]
for trial in top_5:
    print(f"Trial {trial.number}: {trial.value:.4f}")
```

### Compare learning rates across backbones
```python
import pandas as pd

data = []
for trial in study.trials:
    if trial.state.name == 'COMPLETE':
        data.append({
            'trial': trial.number,
            'backbone': trial.params.get('backbone'),
            'lr': trial.params.get('lr'),
            'val_acc': trial.value
        })

df = pd.DataFrame(data)
print(df.groupby('backbone')['val_acc'].describe())
```

## ğŸš¨ Troubleshooting

**Issue**: MLflow UI shows no runs
- Check you're in the correct directory with `mlruns/`
- Verify experiment name in MLflow UI matches your code

**Issue**: Optuna dashboard shows empty database
- Verify database path: `sqlite:///runs/experiments.db`
- Check database file exists and has correct permissions

**Issue**: Study.pkl file not found
- Ensure optimization completed successfully
- Check `runs/{experiment_name}/optimize/` directory

**Issue**: Metrics.json missing for some trials
- Trials may have failed or been pruned
- Check trial state: `trial.state.name` (COMPLETE, PRUNED, FAIL)

## ğŸ“š Additional Resources

- [Optuna Documentation](https://optuna.readthedocs.io/)
- [MLflow Tracking](https://www.mlflow.org/docs/latest/tracking.html)
- [Optuna Visualization](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html)
- [Best Practices for Hyperparameter Tuning](https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/index.html)

---

**Need help?** Check the project's main [README.md](README.md) or [PIPELINE_VCR.md](PIPELINE_VCR.md) for more details on the overall architecture.
